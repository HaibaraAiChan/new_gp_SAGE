main start at this time 1669674913.3751135
-----------------------------------------before load data 
 Nvidia-smi: 0.2530517578125 GB
    Memory Allocated: 0.0  GigaBytes
Max Memory Allocated: 0.0  GigaBytes

#nodes: 2449029
#edges: 123718024
#classes: 47
success----------------------------------------
196571
39255
2164782
# Nodes: 2400608
# Edges: 123718024
# Train: 196571
# Val: 39255
# Test: 2164782
# Classes: 47

----------------------------------------start of run function 
 Nvidia-smi: 0.2530517578125 GB
    Memory Allocated: 0.0  GigaBytes
Max Memory Allocated: 0.0  GigaBytes

in feats:  100
{1: 173, 2: 221, 3: 374, 4: 456, 5: 445, 6: 550, 7: 655, 8: 512, 9: 576, 10: 192609}
v3.py : output nodes local nid:  tensor([130185, 134544, 131319, 134356, 134512, 130186, 130174, 134543, 130354,
        131534], device='cuda:0', dtype=torch.int32)
v3.py : output nodes global nid:  tensor([ 82910, 130045,  47925, 174275, 176347,  64092,   2998,  14770,  80948,
        147379], device='cuda:0')
----------------------------------------- before batch_pred = model(blocks, batch_inputs, deg.to(device)) 
 Nvidia-smi: 1.3663330078125 GB
    Memory Allocated: 0.3225440979003906  GigaBytes
Max Memory Allocated: 0.32397937774658203  GigaBytes

the length of bkt_idx in core.py _bucketing 192609
core.py : output nodes global nid:  tensor([142928, 165022,  70663,  82506,  33441,  50729,  71803, 175954, 106779,
        146894], device='cuda:0', dtype=torch.int32)
----------------------------------------- after batch_pred = model(blocks, batch_inputs, deg.to(device)) 
 Nvidia-smi: 13.9522705078125 GB
    Memory Allocated: 11.458958625793457  GigaBytes
Max Memory Allocated: 12.837597370147705  GigaBytes

degree: tensor(10) # of output: 192609 ratio: 0.9798444328003623 bucket_loss : tensor(4.6964)
----------------------------------------- after loss backward 
 Nvidia-smi: 13.9600830078125 GB
    Memory Allocated: 0.3924589157104492  GigaBytes
Max Memory Allocated: 12.837597370147705  GigaBytes

-------------------------------------------------------loss_sum  :  tensor(4.6964)
----------------------------------------- after optimizer.zero_grad() 
 Nvidia-smi: 13.9600830078125 GB
    Memory Allocated: 0.39313411712646484  GigaBytes
Max Memory Allocated: 12.837597370147705  GigaBytes


 Run 0| Epoch 0 |
{1: 173, 2: 221, 3: 374, 4: 456, 5: 445, 6: 550, 7: 655, 8: 512, 9: 576, 10: 192609}
v3.py : output nodes local nid:  tensor([131453, 130605, 130638, 130639, 131452, 130600, 130211, 131497, 130597,
        130575], device='cuda:0', dtype=torch.int32)
v3.py : output nodes global nid:  tensor([ 85594,  64796,  43905, 174846,  44027,  94274,   6635, 123804, 191519,
         99342], device='cuda:0')
----------------------------------------- before batch_pred = model(blocks, batch_inputs, deg.to(device)) 
 Nvidia-smi: 13.9600830078125 GB
    Memory Allocated: 0.3928065299987793  GigaBytes
Max Memory Allocated: 12.837597370147705  GigaBytes

the length of bkt_idx in core.py _bucketing 192609
core.py : output nodes global nid:  tensor([ 38125,  64477, 116569, 165766, 100350,  87624, 187402, 142538,  93040,
         87818], device='cuda:0', dtype=torch.int32)
----------------------------------------- after batch_pred = model(blocks, batch_inputs, deg.to(device)) 
 Nvidia-smi: 14.6807861328125 GB
    Memory Allocated: 11.494803428649902  GigaBytes
Max Memory Allocated: 12.907317161560059  GigaBytes

degree: tensor(10) # of output: 192609 ratio: 0.9798444328003623 bucket_loss : tensor(4.6336)
----------------------------------------- after loss backward 
 Nvidia-smi: 14.6807861328125 GB
    Memory Allocated: 0.39350032806396484  GigaBytes
Max Memory Allocated: 12.907317161560059  GigaBytes

-------------------------------------------------------loss_sum  :  tensor(4.6336)
----------------------------------------- after optimizer.zero_grad() 
 Nvidia-smi: 14.6807861328125 GB
    Memory Allocated: 0.39350032806396484  GigaBytes
Max Memory Allocated: 12.907317161560059  GigaBytes


 Run 0| Epoch 1 |
{1: 173, 2: 221, 3: 374, 4: 456, 5: 445, 6: 550, 7: 655, 8: 512, 9: 576, 10: 192609}
v3.py : output nodes local nid:  tensor([130698, 130428, 130429, 130697, 130700, 130699, 130701, 130427, 130409,
        130677], device='cuda:0', dtype=torch.int32)
v3.py : output nodes global nid:  tensor([ 58830,  78083,  33872,  54223,  41565, 149776,  26583,  47478, 132306,
        141691], device='cuda:0')
----------------------------------------- before batch_pred = model(blocks, batch_inputs, deg.to(device)) 
 Nvidia-smi: 14.6807861328125 GB
    Memory Allocated: 0.3938279151916504  GigaBytes
Max Memory Allocated: 12.907317161560059  GigaBytes

the length of bkt_idx in core.py _bucketing 192609
core.py : output nodes global nid:  tensor([185562, 178226, 133156,  52180,  94009, 181947, 158023,  14313,  32828,
        168770], device='cuda:0', dtype=torch.int32)
----------------------------------------- after batch_pred = model(blocks, batch_inputs, deg.to(device)) 
 Nvidia-smi: 14.6807861328125 GB
    Memory Allocated: 11.495824813842773  GigaBytes
Max Memory Allocated: 12.908910751342773  GigaBytes

degree: tensor(10) # of output: 192609 ratio: 0.9798444328003623 bucket_loss : tensor(4.5703)
----------------------------------------- after loss backward 
 Nvidia-smi: 14.6807861328125 GB
    Memory Allocated: 0.3938279151916504  GigaBytes
Max Memory Allocated: 12.908910751342773  GigaBytes

-------------------------------------------------------loss_sum  :  tensor(4.5703)
----------------------------------------- after optimizer.zero_grad() 
 Nvidia-smi: 14.6807861328125 GB
    Memory Allocated: 0.3938279151916504  GigaBytes
Max Memory Allocated: 12.908910751342773  GigaBytes


 Run 0| Epoch 2 |
{1: 173, 2: 221, 3: 374, 4: 456, 5: 445, 6: 550, 7: 655, 8: 512, 9: 576, 10: 192609}
v3.py : output nodes local nid:  tensor([130503, 130465, 130403, 130523, 130516, 130467, 131481, 131547, 131503,
        130517], device='cuda:0', dtype=torch.int32)
v3.py : output nodes global nid:  tensor([164711,  89544,   3882, 101283, 119163, 163754, 123521,  77382, 181476,
        185964], device='cuda:0')
----------------------------------------- before batch_pred = model(blocks, batch_inputs, deg.to(device)) 
 Nvidia-smi: 14.6807861328125 GB
    Memory Allocated: 0.3933067321777344  GigaBytes
Max Memory Allocated: 12.908910751342773  GigaBytes

the length of bkt_idx in core.py _bucketing 192609
core.py : output nodes global nid:  tensor([132943, 147509,  30629,   6151,  28732, 101524, 179002, 109180, 117450,
        151594], device='cuda:0', dtype=torch.int32)
----------------------------------------- after batch_pred = model(blocks, batch_inputs, deg.to(device)) 
 Nvidia-smi: 14.6807861328125 GB
    Memory Allocated: 11.495303630828857  GigaBytes
Max Memory Allocated: 12.908910751342773  GigaBytes

degree: tensor(10) # of output: 192609 ratio: 0.9798444328003623 bucket_loss : tensor(4.5078)
----------------------------------------- after loss backward 
 Nvidia-smi: 14.6807861328125 GB
    Memory Allocated: 0.39261293411254883  GigaBytes
Max Memory Allocated: 12.908910751342773  GigaBytes

-------------------------------------------------------loss_sum  :  tensor(4.5078)
----------------------------------------- after optimizer.zero_grad() 
 Nvidia-smi: 14.6807861328125 GB
    Memory Allocated: 0.39261293411254883  GigaBytes
Max Memory Allocated: 12.908910751342773  GigaBytes


 Run 0| Epoch 3 |
{1: 173, 2: 221, 3: 374, 4: 456, 5: 445, 6: 550, 7: 655, 8: 512, 9: 576, 10: 192609}
v3.py : output nodes local nid:  tensor([130182, 131407, 130199, 131408, 131665, 130307, 131640, 131624, 131623,
        130320], device='cuda:0', dtype=torch.int32)
v3.py : output nodes global nid:  tensor([183840,  14224, 178868, 182117, 108296,   9554, 132128, 131441,  32212,
        133940], device='cuda:0')
----------------------------------------- before batch_pred = model(blocks, batch_inputs, deg.to(device)) 
 Nvidia-smi: 14.6807861328125 GB
    Memory Allocated: 0.3936939239501953  GigaBytes
Max Memory Allocated: 12.908910751342773  GigaBytes

the length of bkt_idx in core.py _bucketing 192609
core.py : output nodes global nid:  tensor([136708, 167585,  80378,  14694, 135792, 193887,  87514, 107903,   4410,
         91084], device='cuda:0', dtype=torch.int32)
----------------------------------------- after batch_pred = model(blocks, batch_inputs, deg.to(device)) 
 Nvidia-smi: 14.6807861328125 GB
    Memory Allocated: 11.495690822601318  GigaBytes
Max Memory Allocated: 12.908910751342773  GigaBytes

degree: tensor(10) # of output: 192609 ratio: 0.9798444328003623 bucket_loss : tensor(4.4446)
----------------------------------------- after loss backward 
 Nvidia-smi: 14.6807861328125 GB
    Memory Allocated: 0.3936939239501953  GigaBytes
Max Memory Allocated: 12.908910751342773  GigaBytes

-------------------------------------------------------loss_sum  :  tensor(4.4446)
----------------------------------------- after optimizer.zero_grad() 
 Nvidia-smi: 14.6807861328125 GB
    Memory Allocated: 0.3936939239501953  GigaBytes
Max Memory Allocated: 12.908910751342773  GigaBytes


 Run 0| Epoch 4 |
{1: 173, 2: 221, 3: 374, 4: 456, 5: 445, 6: 550, 7: 655, 8: 512, 9: 576, 10: 192609}
v3.py : output nodes local nid:  tensor([131375, 131534, 131428, 131693, 131376, 131427, 131384, 131414, 131535,
        131413], device='cuda:0', dtype=torch.int32)
v3.py : output nodes global nid:  tensor([ 74719,  53417,  76261,  60432, 147978,  65580, 183923,  87835,  55561,
          9022], device='cuda:0')
----------------------------------------- before batch_pred = model(blocks, batch_inputs, deg.to(device)) 
 Nvidia-smi: 14.6807861328125 GB
    Memory Allocated: 0.3928036689758301  GigaBytes
Max Memory Allocated: 12.908910751342773  GigaBytes

the length of bkt_idx in core.py _bucketing 192609
core.py : output nodes global nid:  tensor([169792, 170500,  94941, 148501, 149790,  47238, 135800, 117415,  24497,
        150330], device='cuda:0', dtype=torch.int32)
----------------------------------------- after batch_pred = model(blocks, batch_inputs, deg.to(device)) 
 Nvidia-smi: 14.6807861328125 GB
    Memory Allocated: 11.494800567626953  GigaBytes
Max Memory Allocated: 12.908910751342773  GigaBytes

degree: tensor(10) # of output: 192609 ratio: 0.9798444328003623 bucket_loss : tensor(4.3810)
----------------------------------------- after loss backward 
 Nvidia-smi: 14.6807861328125 GB
    Memory Allocated: 0.3928036689758301  GigaBytes
Max Memory Allocated: 12.908910751342773  GigaBytes

-------------------------------------------------------loss_sum  :  tensor(4.3810)
----------------------------------------- after optimizer.zero_grad() 
 Nvidia-smi: 14.6807861328125 GB
    Memory Allocated: 0.3928036689758301  GigaBytes
Max Memory Allocated: 12.908910751342773  GigaBytes


 Run 0| Epoch 5 |
{1: 173, 2: 221, 3: 374, 4: 456, 5: 445, 6: 550, 7: 655, 8: 512, 9: 576, 10: 192609}
v3.py : output nodes local nid:  tensor([131852, 130651, 124716, 132314, 131847, 124631, 130570, 125015, 131903,
        130569], device='cuda:0', dtype=torch.int32)
v3.py : output nodes global nid:  tensor([192459,   3209, 166394, 159980, 157265, 178441,  21658,  11744, 157259,
         99169], device='cuda:0')
----------------------------------------- before batch_pred = model(blocks, batch_inputs, deg.to(device)) 
 Nvidia-smi: 14.6807861328125 GB
    Memory Allocated: 0.39316368103027344  GigaBytes
Max Memory Allocated: 12.908910751342773  GigaBytes

the length of bkt_idx in core.py _bucketing 192609
core.py : output nodes global nid:  tensor([ 51802, 103403, 142529,  62736, 193509,  38861,  72809,  94198, 122978,
        193765], device='cuda:0', dtype=torch.int32)
----------------------------------------- after batch_pred = model(blocks, batch_inputs, deg.to(device)) 
 Nvidia-smi: 14.6807861328125 GB
    Memory Allocated: 11.495160579681396  GigaBytes
Max Memory Allocated: 12.908910751342773  GigaBytes

degree: tensor(10) # of output: 192609 ratio: 0.9798444328003623 bucket_loss : tensor(4.3164)
----------------------------------------- after loss backward 
 Nvidia-smi: 14.6807861328125 GB
    Memory Allocated: 0.3936758041381836  GigaBytes
Max Memory Allocated: 12.908910751342773  GigaBytes

-------------------------------------------------------loss_sum  :  tensor(4.3164)
----------------------------------------- after optimizer.zero_grad() 
 Nvidia-smi: 14.6807861328125 GB
    Memory Allocated: 0.3936758041381836  GigaBytes
Max Memory Allocated: 12.908910751342773  GigaBytes


 Run 0| Epoch 6 |
{1: 173, 2: 221, 3: 374, 4: 456, 5: 445, 6: 550, 7: 655, 8: 512, 9: 576, 10: 192609}
v3.py : output nodes local nid:  tensor([132022, 132038, 132027, 125003, 132026, 132025, 132024, 132023, 132035,
        131975], device='cuda:0', dtype=torch.int32)
v3.py : output nodes global nid:  tensor([125129, 193044,  32931,  95032,  10202, 134120,  82082,  32255, 120242,
         94564], device='cuda:0')
----------------------------------------- before batch_pred = model(blocks, batch_inputs, deg.to(device)) 
 Nvidia-smi: 14.6807861328125 GB
    Memory Allocated: 0.3928532600402832  GigaBytes
Max Memory Allocated: 12.908910751342773  GigaBytes

the length of bkt_idx in core.py _bucketing 192609
core.py : output nodes global nid:  tensor([ 58092, 105281,  21091, 101410,  18010,  23599,  67383,  80535, 150840,
         19171], device='cuda:0', dtype=torch.int32)
----------------------------------------- after batch_pred = model(blocks, batch_inputs, deg.to(device)) 
 Nvidia-smi: 14.6807861328125 GB
    Memory Allocated: 11.494850158691406  GigaBytes
Max Memory Allocated: 12.908910751342773  GigaBytes

degree: tensor(10) # of output: 192609 ratio: 0.9798444328003623 bucket_loss : tensor(4.2505)
----------------------------------------- after loss backward 
 Nvidia-smi: 14.6807861328125 GB
    Memory Allocated: 0.39234113693237305  GigaBytes
Max Memory Allocated: 12.908910751342773  GigaBytes

-------------------------------------------------------loss_sum  :  tensor(4.2505)
----------------------------------------- after optimizer.zero_grad() 
 Nvidia-smi: 14.6807861328125 GB
    Memory Allocated: 0.39234113693237305  GigaBytes
Max Memory Allocated: 12.908910751342773  GigaBytes


 Run 0| Epoch 7 |
{1: 173, 2: 221, 3: 374, 4: 456, 5: 445, 6: 550, 7: 655, 8: 512, 9: 576, 10: 192609}
v3.py : output nodes local nid:  tensor([131724, 131758, 131847, 131759, 131915, 131740, 131908, 131830, 131910,
        131805], device='cuda:0', dtype=torch.int32)
v3.py : output nodes global nid:  tensor([150283,  95513,  97646, 111619, 104810,  91187,  82020, 118824, 171295,
        128798], device='cuda:0')
----------------------------------------- before batch_pred = model(blocks, batch_inputs, deg.to(device)) 
 Nvidia-smi: 14.6807861328125 GB
    Memory Allocated: 0.3928494453430176  GigaBytes
Max Memory Allocated: 12.908910751342773  GigaBytes

the length of bkt_idx in core.py _bucketing 192609
core.py : output nodes global nid:  tensor([ 41397, 113839, 168800, 153860,   7679, 109485,  71059, 125115, 112870,
         15472], device='cuda:0', dtype=torch.int32)
----------------------------------------- after batch_pred = model(blocks, batch_inputs, deg.to(device)) 
 Nvidia-smi: 14.6807861328125 GB
    Memory Allocated: 11.49484634399414  GigaBytes
Max Memory Allocated: 12.908910751342773  GigaBytes

degree: tensor(10) # of output: 192609 ratio: 0.9798444328003623 bucket_loss : tensor(4.1837)
----------------------------------------- after loss backward 
 Nvidia-smi: 14.6807861328125 GB
    Memory Allocated: 0.3928494453430176  GigaBytes
Max Memory Allocated: 12.908910751342773  GigaBytes

-------------------------------------------------------loss_sum  :  tensor(4.1837)
----------------------------------------- after optimizer.zero_grad() 
 Nvidia-smi: 14.6807861328125 GB
    Memory Allocated: 0.3928494453430176  GigaBytes
Max Memory Allocated: 12.908910751342773  GigaBytes


 Run 0| Epoch 8 |
{1: 173, 2: 221, 3: 374, 4: 456, 5: 445, 6: 550, 7: 655, 8: 512, 9: 576, 10: 192609}
v3.py : output nodes local nid:  tensor([131459, 131449, 131458, 131460, 131167, 134335, 131445, 131444, 131443,
        134334], device='cuda:0', dtype=torch.int32)
v3.py : output nodes global nid:  tensor([157639,  41825,  11962, 133176, 108292,  71690, 137059,  51115,  78111,
        144168], device='cuda:0')
----------------------------------------- before batch_pred = model(blocks, batch_inputs, deg.to(device)) 
 Nvidia-smi: 14.6807861328125 GB
    Memory Allocated: 0.3933887481689453  GigaBytes
Max Memory Allocated: 12.908910751342773  GigaBytes

the length of bkt_idx in core.py _bucketing 192609
core.py : output nodes global nid:  tensor([170531, 121455, 120101,  90778, 120409,  99087, 190662, 192401,  68941,
          7949], device='cuda:0', dtype=torch.int32)
----------------------------------------- after batch_pred = model(blocks, batch_inputs, deg.to(device)) 
 Nvidia-smi: 14.6807861328125 GB
    Memory Allocated: 11.495385646820068  GigaBytes
Max Memory Allocated: 12.908910751342773  GigaBytes

degree: tensor(10) # of output: 192609 ratio: 0.9798444328003623 bucket_loss : tensor(4.1145)
----------------------------------------- after loss backward 
 Nvidia-smi: 14.6807861328125 GB
    Memory Allocated: 0.3933887481689453  GigaBytes
Max Memory Allocated: 12.908910751342773  GigaBytes

-------------------------------------------------------loss_sum  :  tensor(4.1145)
----------------------------------------- after optimizer.zero_grad() 
 Nvidia-smi: 14.6807861328125 GB
    Memory Allocated: 0.3933887481689453  GigaBytes
Max Memory Allocated: 12.908910751342773  GigaBytes


 Run 0| Epoch 9 |
