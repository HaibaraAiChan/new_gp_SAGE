main start at this time 1657858230.827328
-----------------------------------------before load data 
 Nvidia-smi: 0.1717529296875 GB
    Memory Allocated: 0.0  GigaBytes
Max Memory Allocated: 0.0  GigaBytes

#nodes: 2449029
#edges: 123718024
#classes: 47
success----------------------------------------
196571
39255
2164782
# Nodes: 2400608
# Edges: 123718024
# Train: 196571
# Val: 39255
# Test: 2164782
# Classes: 47

----------------------------------------start of run function 
 Nvidia-smi: 0.1717529296875 GB
    Memory Allocated: 0.0  GigaBytes
Max Memory Allocated: 0.0  GigaBytes

in feats:  100
----------------------------------------before model to device 
 Nvidia-smi: 0.1717529296875 GB
    Memory Allocated: 0.0  GigaBytes
Max Memory Allocated: 0.0  GigaBytes

----------------------------------------after model to device
 Nvidia-smi: 1.0174560546875 GB
    Memory Allocated: 3.528594970703125e-05  GigaBytes
Max Memory Allocated: 3.528594970703125e-05  GigaBytes

----------------------------------------before generate dataloader block 
 Nvidia-smi: 1.0174560546875 GB
    Memory Allocated: 3.528594970703125e-05  GigaBytes
Max Memory Allocated: 3.528594970703125e-05  GigaBytes

The real block id is  0
get_global_graph_edges_ids_block function  spend 0.06351470947265625
global_2_local spend time (sec) 0.27663111686706543
----------------------------  graph partition start---------------------
g = dgl.graph((u,v))  spent  0.011642217636108398
the counter of in-degree current block !!!!!!!!!!!!!!_______________!!!!!!!!!!
Counter({0: 652869, 10: 192609, 7: 655, 9: 576, 6: 550, 8: 512, 4: 456, 5: 445, 3: 374, 2: 221, 1: 173})

A = g.adjacency_matrix() spent  0.03394818305969238
auxiliary_graph
Graph(num_nodes=849440, num_edges=11360241,
      ndata_schemes={}
      edata_schemes={'w': Scheme(shape=(), dtype=torch.float32)})
get remove nodes spent  0.41985511779785156
remove nodes length  652869

auxiliary_graph.remove_nodes spent  0.6962020397186279
after remove non output nodes the auxiliary_graph
Graph(num_nodes=196571, num_edges=11360241,
      ndata_schemes={}
      edata_schemes={'w': Scheme(shape=(), dtype=torch.float32)})
auxiliary_graph_no_diag generation spent  0.4960482120513916

the counter of shared neighbor distribution
Counter({1.0: 10357862, 2.0: 623144, 3.0: 111270, 4.0: 35606, 5.0: 13370, 6.0: 7938, 7.0: 6402, 9.0: 3186, 8.0: 3046, 10.0: 1846})
11163670
Convert a graph into a bidirected graph: 0.603 seconds
Metis partitioning: 2.275 seconds
Split the graph: 1.466 seconds
Construct subgraphs: 0.078 seconds
auxiliary_graph_no_diag dgl.metis_partition spent  4.427513837814331
97198
99373
total k batches seeds list generation spend  8.818239212036133
after graph partition
graph partition algorithm spend time 9.192373275756836
97198
99373
partition_len_list
[449514, 432482]
REG selection method  spend 9.66160249710083
time for parepare:  0.1692948341369629
local_output_nid generation:  0.014475345611572266
local_in_edges_tensor generation:  0.014171123504638672
mini_batch_src_global generation:  0.030637264251708984
r_  generation:  0.45816659927368164
local_output_nid generation:  0.014663457870483398
local_in_edges_tensor generation:  0.025171995162963867
mini_batch_src_global generation:  0.049019575119018555
r_  generation:  0.5267119407653809
----------------------check_connections_block total spend ----------------------------- 1.5152249336242676
generate_one_block  2.01420521736145
generate_one_block  0.5759589672088623
----------===============-------------===============-------------the number of batches *****---- 2

original number of batches:  2
re graph partition time:  0

-----------------------------------------after block dataloader generation 
 Nvidia-smi: 1.0174560546875 GB
    Memory Allocated: 3.528594970703125e-05  GigaBytes
Max Memory Allocated: 3.528594970703125e-05  GigaBytes

connection checking time:  1.5152249336242676
block generation total time  2.5901641845703125
average batch blocks generation time:  1.2950820922851562
block dataloader generation time/epoch 13.972437858581543
pseudo mini batch 0 input nodes size: 449514
----------------------------------------before load block subtensor 
 Nvidia-smi: 1.0174560546875 GB
    Memory Allocated: 3.528594970703125e-05  GigaBytes
Max Memory Allocated: 3.528594970703125e-05  GigaBytes

----------------------------------------before batch input features to device
 Nvidia-smi: 1.0174560546875 GB
    Memory Allocated: 3.528594970703125e-05  GigaBytes
Max Memory Allocated: 3.528594970703125e-05  GigaBytes

----------------------------------------after batch input features to device
 Nvidia-smi: 1.1854248046875 GB
    Memory Allocated: 0.16800403594970703  GigaBytes
Max Memory Allocated: 0.16800403594970703  GigaBytes

----------------------------------------after  batch labels to device
 Nvidia-smi: 1.1854248046875 GB
    Memory Allocated: 0.16872835159301758  GigaBytes
Max Memory Allocated: 0.16872835159301758  GigaBytes

----------------------------------------after load block subtensor  
 Nvidia-smi: 1.1854248046875 GB
    Memory Allocated: 0.16872835159301758  GigaBytes
Max Memory Allocated: 0.16872835159301758  GigaBytes

----------------------------------------after blocks to device 
 Nvidia-smi: 1.2694091796875 GB
    Memory Allocated: 0.17590761184692383  GigaBytes
Max Memory Allocated: 0.17590761184692383  GigaBytes

----------------------------------------before batch_pred = model(blocks, batch_inputs) 
 Nvidia-smi: 1.2694091796875 GB
    Memory Allocated: 0.17590761184692383  GigaBytes
Max Memory Allocated: 0.17590761184692383  GigaBytes

----------------input nodes number: 449514
----------------output nodes number: 97198
----------------edges number: 963549
----------------------------------------before mean aggregator
 Nvidia-smi: 1.2694091796875 GB
    Memory Allocated: 0.17998123168945312  GigaBytes
Max Memory Allocated: 0.18357086181640625  GigaBytes

----------------------------------------after mean aggregator-------
 Nvidia-smi: 1.5740966796875 GB
    Memory Allocated: 0.24165058135986328  GigaBytes
Max Memory Allocated: 0.2621040344238281  GigaBytes

torch.Size([97198, 47])
torch.Size([97198, 47])
----------------------------------------after rst
 Nvidia-smi: 1.5740966796875 GB
    Memory Allocated: 0.2586688995361328  GigaBytes
Max Memory Allocated: 0.27568721771240234  GigaBytes

----------------------------------------end of model layers  after x = self.layers[-1](blocks[-1], x)
 Nvidia-smi: 1.5740966796875 GB
    Memory Allocated: 0.24165058135986328  GigaBytes
Max Memory Allocated: 0.27568721771240234  GigaBytes

torch.Size([97198, 47])
----------------------------------------- after batch_pred = model(blocks, batch_inputs) 
 Nvidia-smi: 1.5740966796875 GB
    Memory Allocated: 0.2586688995361328  GigaBytes
Max Memory Allocated: 0.27568721771240234  GigaBytes

-----------------------------------------after loss calculation
 Nvidia-smi: 1.5740966796875 GB
    Memory Allocated: 0.27568817138671875  GigaBytes
Max Memory Allocated: 0.27568864822387695  GigaBytes

-----------------------------------------after loss backward
 Nvidia-smi: 1.6092529296875 GB
    Memory Allocated: 0.20457744598388672  GigaBytes
Max Memory Allocated: 0.31084442138671875  GigaBytes

pseudo mini batch 1 input nodes size: 432482
----------------------------------------before load block subtensor 
 Nvidia-smi: 1.6092529296875 GB
    Memory Allocated: 0.18578290939331055  GigaBytes
Max Memory Allocated: 0.31084442138671875  GigaBytes

----------------------------------------before batch input features to device
 Nvidia-smi: 1.6092529296875 GB
    Memory Allocated: 0.18578290939331055  GigaBytes
Max Memory Allocated: 0.31084442138671875  GigaBytes

----------------------------------------after batch input features to device
 Nvidia-smi: 1.7713623046875 GB
    Memory Allocated: 0.3468952178955078  GigaBytes
Max Memory Allocated: 0.3468952178955078  GigaBytes

----------------------------------------after  batch labels to device
 Nvidia-smi: 1.7713623046875 GB
    Memory Allocated: 0.34763574600219727  GigaBytes
Max Memory Allocated: 0.34763574600219727  GigaBytes

----------------------------------------after load block subtensor  
 Nvidia-smi: 1.7713623046875 GB
    Memory Allocated: 0.17894268035888672  GigaBytes
Max Memory Allocated: 0.34763574600219727  GigaBytes

----------------------------------------after blocks to device 
 Nvidia-smi: 1.7713623046875 GB
    Memory Allocated: 0.18628597259521484  GigaBytes
Max Memory Allocated: 0.34763574600219727  GigaBytes

----------------------------------------before batch_pred = model(blocks, batch_inputs) 
 Nvidia-smi: 1.7713623046875 GB
    Memory Allocated: 0.18628597259521484  GigaBytes
Max Memory Allocated: 0.34763574600219727  GigaBytes

----------------input nodes number: 432482
----------------output nodes number: 99373
----------------edges number: 985492
----------------------------------------before mean aggregator
 Nvidia-smi: 1.7713623046875 GB
    Memory Allocated: 0.19024896621704102  GigaBytes
Max Memory Allocated: 0.34763574600219727  GigaBytes

----------------------------------------after mean aggregator-------
 Nvidia-smi: 1.7713623046875 GB
    Memory Allocated: 0.2523818016052246  GigaBytes
Max Memory Allocated: 0.34763574600219727  GigaBytes

torch.Size([99373, 47])
torch.Size([99373, 47])
----------------------------------------after rst
 Nvidia-smi: 1.7713623046875 GB
    Memory Allocated: 0.26978111267089844  GigaBytes
Max Memory Allocated: 0.34763574600219727  GigaBytes

----------------------------------------end of model layers  after x = self.layers[-1](blocks[-1], x)
 Nvidia-smi: 1.7713623046875 GB
    Memory Allocated: 0.2523818016052246  GigaBytes
Max Memory Allocated: 0.34763574600219727  GigaBytes

torch.Size([99373, 47])
----------------------------------------- after batch_pred = model(blocks, batch_inputs) 
 Nvidia-smi: 1.7713623046875 GB
    Memory Allocated: 0.2527627944946289  GigaBytes
Max Memory Allocated: 0.34763574600219727  GigaBytes

-----------------------------------------after loss calculation
 Nvidia-smi: 1.7713623046875 GB
    Memory Allocated: 0.27016258239746094  GigaBytes
Max Memory Allocated: 0.34763574600219727  GigaBytes

-----------------------------------------after loss backward
 Nvidia-smi: 1.7713623046875 GB
    Memory Allocated: 0.19834375381469727  GigaBytes
Max Memory Allocated: 0.34763574600219727  GigaBytes

-----------------------------------------after optimizer step
 Nvidia-smi: 1.7713623046875 GB
    Memory Allocated: 0.19841432571411133  GigaBytes
Max Memory Allocated: 0.34763574600219727  GigaBytes

-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.7713623046875 GB
    Memory Allocated: 0.19841432571411133  GigaBytes
Max Memory Allocated: 0.34763574600219727  GigaBytes

times | data loading | block to device | model prediction | loss calculation | loss backward |  optimizer step |
      |0.15281808376312256 |0.173925518989563 |0.3898698091506958 |0.00023877620697021484 |0.0033251047134399414 |0.0012426376342773438 |
----------------------------------------------------------pseudo_mini_loss sum 5.122900485992432
Total (block generation + training)time/epoch 15.425500392913818
Training time/epoch 1.4527337551116943
Training time without block to device /epoch 1.1048827171325684
Training time without total dataloading part /epoch 0.7881100177764893
load block tensor time/epoch 0.3056361675262451
block to device time/epoch 0.347851037979126
input features size transfer per epoch 2.682209014892578e-07
blocks size to device per epoch 1.7881393432617188e-07
 Run 0| Epoch 0 |
Number of nodes for computation during this epoch:  881996
Number of first layer input nodes during this epoch:  881996
Number of first layer output nodes during this epoch:  196571
GraphSAGE(
  (layers): ModuleList(
    (0): SAGEConv(
      (fc_self): Linear(in_features=100, out_features=47, bias=False)
      (fc_neigh): Linear(in_features=100, out_features=47, bias=False)
    )
  )
  (dropout): Dropout(p=0.5, inplace=False)
)
total model parameters size  9400
trainable parameters
layers.0.fc_self.weight, torch.Size([47, 100])
layers.0.fc_neigh.weight, torch.Size([47, 100])
----------------------------------------
un-trainable parameters
