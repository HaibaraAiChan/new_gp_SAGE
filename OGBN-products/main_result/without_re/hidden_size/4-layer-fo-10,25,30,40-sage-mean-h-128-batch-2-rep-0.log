main start at this time 1656049363.7867699
-----------------------------------------before load data 
 Nvidia-smi: 0.1717529296875 GB
    Memory Allocated: 0.0  GigaBytes
Max Memory Allocated: 0.0  GigaBytes

#nodes: 2449029
#edges: 123718024
#classes: 47
success----------------------------------------
196571
39255
2164782
# Nodes: 2400608
# Edges: 123718024
# Train: 196571
# Val: 39255
# Test: 2164782
# Classes: 47

----------------------------------------start of run function 
 Nvidia-smi: 0.1717529296875 GB
    Memory Allocated: 0.0  GigaBytes
Max Memory Allocated: 0.0  GigaBytes

in feats:  100
----------------------------------------before model to device 
 Nvidia-smi: 0.1717529296875 GB
    Memory Allocated: 0.0  GigaBytes
Max Memory Allocated: 0.0  GigaBytes

----------------------------------------after model to device
 Nvidia-smi: 1.0174560546875 GB
    Memory Allocated: 0.00038433074951171875  GigaBytes
Max Memory Allocated: 0.00038433074951171875  GigaBytes

----------------------------------------before generate dataloader block 
 Nvidia-smi: 1.0174560546875 GB
    Memory Allocated: 0.00038433074951171875  GigaBytes
Max Memory Allocated: 0.00038433074951171875  GigaBytes

The real block id is  3
get_global_graph_edges_ids_block function  spend 0.31974220275878906
global_2_local spend time (sec) 0.4664332866668701
----------------------------  graph partition start---------------------
g = dgl.graph((u,v))  spent  0.08138561248779297
the counter of in-degree current block !!!!!!!!!!!!!!_______________!!!!!!!!!!
Counter({0: 1181171, 40: 172537, 39: 816, 33: 793, 37: 791, 38: 775, 28: 768, 35: 754, 36: 741, 32: 727, 14: 695, 31: 691, 30: 689, 34: 677, 15: 673, 25: 671, 26: 659, 7: 655, 23: 648, 27: 646, 24: 645, 29: 643, 10: 640, 11: 624, 13: 615, 16: 614, 18: 604, 22: 592, 21: 586, 19: 584, 12: 583, 9: 576, 20: 572, 17: 556, 6: 550, 8: 512, 4: 456, 5: 445, 3: 374, 2: 221, 1: 173})

A = g.adjacency_matrix() spent  0.12597298622131348
auxiliary_graph
Graph(num_nodes=1377742, num_edges=101873543,
      ndata_schemes={}
      edata_schemes={'w': Scheme(shape=(), dtype=torch.float32)})
get remove nodes spent  0.705068826675415
remove nodes length  1181171

auxiliary_graph.remove_nodes spent  5.888795852661133
after remove non output nodes the auxiliary_graph
Graph(num_nodes=196571, num_edges=101873543,
      ndata_schemes={}
      edata_schemes={'w': Scheme(shape=(), dtype=torch.float32)})
auxiliary_graph_no_diag generation spent  4.481164216995239

the counter of shared neighbor distribution
Counter({1.0: 76625834, 2.0: 14048222, 3.0: 5011620, 4.0: 2308328, 5.0: 1244288, 6.0: 737614, 7.0: 469104, 8.0: 320932, 9.0: 223716, 10.0: 162494, 11.0: 118854, 12.0: 89870, 13.0: 69466, 14.0: 53028, 15.0: 41764, 16.0: 31248, 17.0: 22002, 18.0: 19492, 19.0: 13018, 20.0: 10882, 21.0: 7144, 22.0: 5740, 23.0: 4796, 24.0: 3814, 25.0: 3584, 33.0: 3046, 28.0: 2926, 26.0: 2688, 30.0: 2312, 27.0: 2286, 31.0: 2154, 39.0: 2098, 32.0: 2040, 35.0: 1750, 29.0: 1680, 37.0: 1650, 36.0: 1526, 38.0: 1460, 34.0: 1422, 40.0: 1080})
101676972
Convert a graph into a bidirected graph: 7.713 seconds
Metis partitioning: 22.291 seconds
Split the graph: 12.416 seconds
Construct subgraphs: 0.067 seconds
auxiliary_graph_no_diag dgl.metis_partition spent  42.52974033355713
100369
96202
total k batches seeds list generation spend  75.17160367965698
after graph partition
graph partition algorithm spend time 76.48259806632996
100369
96202
partition_len_list
[720831, 765107]
REG selection method  spend 77.22475957870483
time for parepare:  0.275714635848999
local_output_nid generation:  0.013714075088500977
local_in_edges_tensor generation:  0.08192181587219238
mini_batch_src_global generation:  0.11063075065612793
r_  generation:  1.7616970539093018
local_output_nid generation:  0.01620030403137207
local_in_edges_tensor generation:  0.09420299530029297
mini_batch_src_global generation:  0.15184354782104492
r_  generation:  1.725564956665039
----------------------check_connections_block total spend ----------------------------- 4.85810923576355
generate_one_block  3.458861827850342
generate_one_block  2.2380850315093994
The real block id is  2
get_global_graph_edges_ids_block function  spend 2.107156276702881
gen group dst list time:  0.04031729698181152
time for parepare:  0.41791796684265137
local_output_nid generation:  0.11546444892883301
local_in_edges_tensor generation:  0.4557168483734131
mini_batch_src_global generation:  0.5567517280578613
r_  generation:  7.720616579055786
local_output_nid generation:  0.13712143898010254
local_in_edges_tensor generation:  0.40840673446655273
mini_batch_src_global generation:  0.7597768306732178
r_  generation:  8.897186994552612
----------------------check_connections_block total spend ----------------------------- 22.32412314414978
generate_one_block  10.313095808029175
generate_one_block  11.029272317886353
The real block id is  1
get_global_graph_edges_ids_block function  spend 2.184492588043213
gen group dst list time:  0.13374018669128418
time for parepare:  0.4691941738128662
local_output_nid generation:  0.3240833282470703
local_in_edges_tensor generation:  0.9038712978363037
mini_batch_src_global generation:  0.891977071762085
r_  generation:  13.483606815338135
local_output_nid generation:  0.3603510856628418
local_in_edges_tensor generation:  0.8497567176818848
mini_batch_src_global generation:  1.2744190692901611
r_  generation:  13.54417634010315
----------------------check_connections_block total spend ----------------------------- 37.07796621322632
generate_one_block  16.68850588798523
generate_one_block  17.42989945411682
The real block id is  0
get_global_graph_edges_ids_block function  spend 1.176513910293579
gen group dst list time:  0.1504518985748291
time for parepare:  0.5065171718597412
local_output_nid generation:  0.5291106700897217
local_in_edges_tensor generation:  0.9090523719787598
mini_batch_src_global generation:  0.6079068183898926
r_  generation:  9.909570455551147
local_output_nid generation:  0.5418922901153564
local_in_edges_tensor generation:  0.7908420562744141
mini_batch_src_global generation:  0.7668137550354004
r_  generation:  9.230914831161499
----------------------check_connections_block total spend ----------------------------- 27.668383598327637
generate_one_block  12.306359767913818
generate_one_block  11.87957763671875
----------===============-------------===============-------------the number of batches *****---- 2

original number of batches:  2
re graph partition time:  0

-----------------------------------------after block dataloader generation 
 Nvidia-smi: 1.0174560546875 GB
    Memory Allocated: 0.00038433074951171875  GigaBytes
Max Memory Allocated: 0.00038433074951171875  GigaBytes

connection checking time:  91.92858219146729
block generation total time  85.34365773200989
average batch blocks generation time:  10.667957216501236
block dataloader generation time/epoch 267.9747245311737
pseudo mini batch 0 input nodes size: 2207973
----------------------------------------before load block subtensor 
 Nvidia-smi: 1.0174560546875 GB
    Memory Allocated: 0.00038433074951171875  GigaBytes
Max Memory Allocated: 0.00038433074951171875  GigaBytes

----------------------------------------before batch input features to device
 Nvidia-smi: 1.0174560546875 GB
    Memory Allocated: 0.00038433074951171875  GigaBytes
Max Memory Allocated: 0.00038433074951171875  GigaBytes

----------------------------------------after batch input features to device
 Nvidia-smi: 1.8416748046875 GB
    Memory Allocated: 0.8229184150695801  GigaBytes
Max Memory Allocated: 0.8229184150695801  GigaBytes

----------------------------------------after  batch labels to device
 Nvidia-smi: 1.8416748046875 GB
    Memory Allocated: 0.8236665725708008  GigaBytes
Max Memory Allocated: 0.8236665725708008  GigaBytes

----------------------------------------after load block subtensor  
 Nvidia-smi: 1.8416748046875 GB
    Memory Allocated: 0.8236665725708008  GigaBytes
Max Memory Allocated: 0.8236665725708008  GigaBytes

----------------------------------------after blocks to device 
 Nvidia-smi: 2.4295654296875 GB
    Memory Allocated: 1.3377866744995117  GigaBytes
Max Memory Allocated: 1.3377866744995117  GigaBytes

----------------------------------------before batch_pred = model(blocks, batch_inputs) 
 Nvidia-smi: 2.4295654296875 GB
    Memory Allocated: 1.3377866744995117  GigaBytes
Max Memory Allocated: 1.3377866744995117  GigaBytes

first layer input nodes number: 2207973
first layer output nodes number: 2063446
edges number: 18523004
torch.Size([2207973, 100])
torch.Size([2063446, 128])
input nodes number: 2063446
output nodes number: 1466261
edges number: 28635674
torch.Size([2063446, 128])
torch.Size([1466261, 128])
input nodes number: 1466261
output nodes number: 720831
edges number: 17844092
torch.Size([1466261, 128])
torch.Size([720831, 128])
torch.Size([100369, 47])
-----------------------------------------batch_pred = model(blocks, batch_inputs) 
 Nvidia-smi: 11.0252685546875 GB
    Memory Allocated: 8.406605243682861  GigaBytes
Max Memory Allocated: 8.638247966766357  GigaBytes

-----------------------------------------after loss calculation
 Nvidia-smi: 11.0252685546875 GB
    Memory Allocated: 8.424180030822754  GigaBytes
Max Memory Allocated: 8.638247966766357  GigaBytes

-----------------------------------------after loss backward
 Nvidia-smi: 15.3651123046875 GB
    Memory Allocated: 2.350674629211426  GigaBytes
Max Memory Allocated: 9.060046195983887  GigaBytes

pseudo mini batch 1 input nodes size: 2169644
----------------------------------------before load block subtensor 
 Nvidia-smi: 15.3651123046875 GB
    Memory Allocated: 0.8416256904602051  GigaBytes
Max Memory Allocated: 9.060046195983887  GigaBytes

----------------------------------------before batch input features to device
 Nvidia-smi: 15.3651123046875 GB
    Memory Allocated: 0.8416256904602051  GigaBytes
Max Memory Allocated: 9.060046195983887  GigaBytes

----------------------------------------after batch input features to device
 Nvidia-smi: 15.3651123046875 GB
    Memory Allocated: 1.649881362915039  GigaBytes
Max Memory Allocated: 9.060046195983887  GigaBytes

----------------------------------------after  batch labels to device
 Nvidia-smi: 15.3651123046875 GB
    Memory Allocated: 1.6505985260009766  GigaBytes
Max Memory Allocated: 9.060046195983887  GigaBytes

----------------------------------------after load block subtensor  
 Nvidia-smi: 15.3651123046875 GB
    Memory Allocated: 0.8273162841796875  GigaBytes
Max Memory Allocated: 9.060046195983887  GigaBytes

----------------------------------------after blocks to device 
 Nvidia-smi: 15.3651123046875 GB
    Memory Allocated: 1.3640012741088867  GigaBytes
Max Memory Allocated: 9.060046195983887  GigaBytes

----------------------------------------before batch_pred = model(blocks, batch_inputs) 
 Nvidia-smi: 15.3651123046875 GB
    Memory Allocated: 1.3640012741088867  GigaBytes
Max Memory Allocated: 9.060046195983887  GigaBytes

first layer input nodes number: 2169644
first layer output nodes number: 1988199
edges number: 18036305
torch.Size([2169644, 100])
torch.Size([1988199, 128])
input nodes number: 1988199
output nodes number: 1466390
edges number: 30112060
torch.Size([1988199, 128])
torch.Size([1466390, 128])
input nodes number: 1466390
output nodes number: 765107
edges number: 20254560
torch.Size([1466390, 128])
torch.Size([765107, 128])
torch.Size([96202, 47])
-----------------------------------------batch_pred = model(blocks, batch_inputs) 
 Nvidia-smi: 15.3651123046875 GB
    Memory Allocated: 8.395493030548096  GigaBytes
Max Memory Allocated: 9.060046195983887  GigaBytes

-----------------------------------------after loss calculation
 Nvidia-smi: 15.3651123046875 GB
    Memory Allocated: 8.41233777999878  GigaBytes
Max Memory Allocated: 9.060046195983887  GigaBytes

-----------------------------------------after loss backward
 Nvidia-smi: 17.0155029296875 GB
    Memory Allocated: 2.410433292388916  GigaBytes
Max Memory Allocated: 9.092245101928711  GigaBytes

-----------------------------------------after optimizer step
 Nvidia-smi: 17.0155029296875 GB
    Memory Allocated: 2.4112019538879395  GigaBytes
Max Memory Allocated: 9.092245101928711  GigaBytes

-----------------------------------------after optimizer zero grad
 Nvidia-smi: 17.0155029296875 GB
    Memory Allocated: 2.4112019538879395  GigaBytes
Max Memory Allocated: 9.092245101928711  GigaBytes

times | data loading | block to device | model prediction | loss calculation | loss backward |  optimizer step |
      |0.3076099157333374 |0.8165278434753418 |0.5266526937484741 |0.00021207332611083984 |0.08266758918762207 |0.0038709640502929688 |
----------------------------------------------------------pseudo_mini_loss sum 12.099868774414062
Total (block generation + training)time/epoch 271.67526268959045
Training time/epoch 3.7001793384552
Training time without block to device /epoch 2.0671236515045166
Training time without total dataloading part /epoch 1.222935676574707
load block tensor time/epoch 0.6152198314666748
block to device time/epoch 1.6330556869506836
input features size transfer per epoch 2.682209014892578e-07
blocks size to device per epoch 1.7881393432617188e-07
 Run 0| Epoch 0 |
Number of nodes for computation during this epoch:  12847851
Number of first layer input nodes during this epoch:  4377617
Number of first layer output nodes during this epoch:  4051645
GraphSAGE(
  (layers): ModuleList(
    (0): SAGEConv(
      (fc_self): Linear(in_features=100, out_features=128, bias=False)
      (fc_neigh): Linear(in_features=100, out_features=128, bias=False)
    )
    (1): SAGEConv(
      (fc_self): Linear(in_features=128, out_features=128, bias=False)
      (fc_neigh): Linear(in_features=128, out_features=128, bias=False)
    )
    (2): SAGEConv(
      (fc_self): Linear(in_features=128, out_features=128, bias=False)
      (fc_neigh): Linear(in_features=128, out_features=128, bias=False)
    )
    (3): SAGEConv(
      (fc_self): Linear(in_features=128, out_features=47, bias=False)
      (fc_neigh): Linear(in_features=128, out_features=47, bias=False)
    )
  )
  (dropout): Dropout(p=0.5, inplace=False)
)
total model parameters size  103168
trainable parameters
layers.0.fc_self.weight, torch.Size([128, 100])
layers.0.fc_neigh.weight, torch.Size([128, 100])
layers.1.fc_self.weight, torch.Size([128, 128])
layers.1.fc_neigh.weight, torch.Size([128, 128])
layers.2.fc_self.weight, torch.Size([128, 128])
layers.2.fc_neigh.weight, torch.Size([128, 128])
layers.3.fc_self.weight, torch.Size([47, 128])
layers.3.fc_neigh.weight, torch.Size([47, 128])
----------------------------------------
un-trainable parameters
