computation info data collection start ...... 
full batch vs pseudo minibatch  compute efficiency (output nodes/time, input nodes /time):
+-------------------------------------------------+-----------------+----------------+-----------------+------------------+------------------+------------------+------------------+
| arxiv sage fan-out 10,25 hidden 256             |     full batch  |        pseudo  |         pseudo  |          pseudo  |          pseudo  |          pseudo  |          pseudo  |
|                                                 |           10,25 |      2 batches |       4 batches |        8 batches |       16 batches |       32 batches |       64 batches |
|                                                 |                 |          10,25 |           10,25 |            10,25 |            10,25 |            10,25 |            10,25 |
+=================================================+=================+================+=================+==================+==================+==================+==================+
| final layer output nodes/pure train time        | 184899          | 137551         |  95630.6        |  53252.8         |  28379.9         |  14994.5         |   7515.39        |
+-------------------------------------------------+-----------------+----------------+-----------------+------------------+------------------+------------------+------------------+
| all layers input nodes//pure train time         | 648259          | 865816         |      1.0041e+06 | 870506           | 675582           | 484903           | 307610           |
+-------------------------------------------------+-----------------+----------------+-----------------+------------------+------------------+------------------+------------------+
| average train time per epoch                    |      0.491841   |      0.661144  |      0.950961   |      1.70772     |      3.20442     |      6.06495     |     12.1006      |
+-------------------------------------------------+-----------------+----------------+-----------------+------------------+------------------+------------------+------------------+
| average number of nodes for computation         | 318840          | 572430         | 954862          |      1.48658e+06 |      2.16485e+06 |      2.94091e+06 |      3.72228e+06 |
+-------------------------------------------------+-----------------+----------------+-----------------+------------------+------------------+------------------+------------------+
| average first layer num of input nodes          | 164246          | 313125         | 572435          | 983614           |      1.56056e+06 |      2.25983e+06 |      2.98689e+06 |
+-------------------------------------------------+-----------------+----------------+-----------------+------------------+------------------+------------------+------------------+
| average first layer num of output nodes         | 154584          | 259335         | 382426          | 502922           | 604334           | 680634           | 734188           |
+-------------------------------------------------+-----------------+----------------+-----------------+------------------+------------------+------------------+------------------+
| CUDA max memory consumption                     |     20.0331     |     13.0231    |      8.51376    |      5.35307     |      3.17328     |      1.80701     |      0.996366    |
+-------------------------------------------------+-----------------+----------------+-----------------+------------------+------------------+------------------+------------------+
| redundancy rate I (First Layer Input)           |      1          |      1.90644   |      3.48523    |      5.98866     |      9.50138     |     13.7588      |     18.1855      |
+-------------------------------------------------+-----------------+----------------+-----------------+------------------+------------------+------------------+------------------+
| redundancy rate O (First Layer Output)          |      1          |      1.67763   |      2.4739     |      3.25339     |      3.90942     |      4.403       |      4.74945     |
+-------------------------------------------------+-----------------+----------------+-----------------+------------------+------------------+------------------+------------------+
| pure graph partition spend time                 |      0          |      0.0141324 |      0.0172052  |      0.0142517   |      0.0154218   |      0.0147313   |      0.0163298   |
+-------------------------------------------------+-----------------+----------------+-----------------+------------------+------------------+------------------+------------------+
| average load block input feature time per epoch |      0.0237163  |      0.145328  |      0.24788    |      0.344144    |      0.383584    |      0.584256    |      0.577597    |
+-------------------------------------------------+-----------------+----------------+-----------------+------------------+------------------+------------------+------------------+
| average block to device time per epoch          |      0.00570309 |      0.0119877 |      0.0154566  |      0.0230416   |      0.0511553   |      0.0727106   |      0.135794    |
+-------------------------------------------------+-----------------+----------------+-----------------+------------------+------------------+------------------+------------------+
| average dataloading time per epoch              |      0.0294194  |      0.157316  |      0.263336   |      0.367186    |      0.434739    |      0.656966    |      0.713391    |
+-------------------------------------------------+-----------------+----------------+-----------------+------------------+------------------+------------------+------------------+
