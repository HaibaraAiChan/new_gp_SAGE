"arxiv sage fan-out 25,35,40 hidden 256","full batch 25,35,40","pseudo 2 batches25,35,40","pseudo 4 batches25,35,40","pseudo 8 batches25,35,40","pseudo 16 batches25,35,40","pseudo 32 batches25,35,40","pseudo 64 batches25,35,40"
final layer output nodes/pure train time,,,,,7384.5641205882575,4026.1214227400474,2052.669627144818
all layers input nodes//pure train time,,,,,211452.76826240166,175065.91186905323,131023.0414813982
average train time per epoch,,,,,12.315012574195862,22.587743997573853,44.30376851558685
average number of nodes for computation,,,,,2604043.5,3954344.0,5804814.5
average first layer num of input nodes,,,,,1523333.0,2487891.3333333335,3922821.0
redundancy rate I (First Layer Input),1.0,,,,9.083029240602938,14.834307225084272,23.390222524327417
redundancy rate O (First Layer Output),1.0,,,,4.725042458576633,6.776252803066219,9.3291784940558
average load block input feature time per epoch,,,,,0.30628323554992676,0.46616458892822266,0.963874101638794
average block to device time per epoch,,,,,0.14051854610443115,0.2009507417678833,0.3206496238708496
average dataloading time per epoch,,,,,0.4468017816543579,0.667115330696106,1.2845237255096436
average first layer num of output nodes,,,,,787348.0,1129147.3333333333,1554549.0
CUDA max memory consumption,,,,,18.618216514587402,14.878828048706055,11.762334823608398
