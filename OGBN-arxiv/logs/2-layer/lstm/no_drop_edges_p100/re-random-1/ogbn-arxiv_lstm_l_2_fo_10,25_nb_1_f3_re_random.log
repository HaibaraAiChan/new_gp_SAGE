Using backend: pytorch
main start at this time 1652218452.0050437
-----------------------------------------before load data 
 Nvidia-smi: 0.2607421875 GB
    Memory Allocated: 0.0  GigaBytes
Max Memory Allocated: 0.0  GigaBytes

ogbn-arxiv
# Nodes: 169343
# Edges: 2315598
# Train: 90941
# Val: 29799
# Test: 48603
# Classes: 40

----------------------------------------start of run function 
 Nvidia-smi: 0.2607421875 GB
    Memory Allocated: 0.0  GigaBytes
Max Memory Allocated: 0.0  GigaBytes

in feats:  128
----------------------------------------before model to device 
 Nvidia-smi: 0.2607421875 GB
    Memory Allocated: 0.0  GigaBytes
Max Memory Allocated: 0.0  GigaBytes

----------------------------------------after model to device
 Nvidia-smi: 0.8232421875 GB
    Memory Allocated: 0.002777576446533203  GigaBytes
Max Memory Allocated: 0.002777576446533203  GigaBytes

----------------------------------------before generate dataloader block 
 Nvidia-smi: 0.8232421875 GB
    Memory Allocated: 0.002777576446533203  GigaBytes
Max Memory Allocated: 0.002777576446533203  GigaBytes

-----------------------------------------after block dataloader generation 
 Nvidia-smi: 0.8232421875 GB
    Memory Allocated: 0.002777576446533203  GigaBytes
Max Memory Allocated: 0.002777576446533203  GigaBytes

connection checking time:  0
block generation total time  0
average batch blocks generation time:  0
pseudo mini batch 0 input nodes size: 164220
----------------------------------------before load block subtensor 
 Nvidia-smi: 0.8232421875 GB
    Memory Allocated: 0.002777576446533203  GigaBytes
Max Memory Allocated: 0.002777576446533203  GigaBytes

----------------------------------------before batch input features to device
 Nvidia-smi: 0.8232421875 GB
    Memory Allocated: 0.002777576446533203  GigaBytes
Max Memory Allocated: 0.002777576446533203  GigaBytes

----------------------------------------after batch input features to device
 Nvidia-smi: 0.9033203125 GB
    Memory Allocated: 0.08108377456665039  GigaBytes
Max Memory Allocated: 0.08108377456665039  GigaBytes

----------------------------------------after  batch labels to device
 Nvidia-smi: 0.9033203125 GB
    Memory Allocated: 0.08176136016845703  GigaBytes
Max Memory Allocated: 0.08176136016845703  GigaBytes

----------------------------------------after load block subtensor  
 Nvidia-smi: 0.9033203125 GB
    Memory Allocated: 0.08176136016845703  GigaBytes
Max Memory Allocated: 0.08176136016845703  GigaBytes

----------------------------------------after blocks to device 
 Nvidia-smi: 0.9716796875 GB
    Memory Allocated: 0.09463882446289062  GigaBytes
Max Memory Allocated: 0.09463882446289062  GigaBytes

----------------------------------------before batch_pred = model(blocks, batch_inputs) 
 Nvidia-smi: 0.9716796875 GB
    Memory Allocated: 0.09463882446289062  GigaBytes
Max Memory Allocated: 0.09463882446289062  GigaBytes

first layer input nodes number: 164220
first layer output nodes number: 154710
edges number: 984029
Traceback (most recent call last):
  File "full_and_pseudo_mini_batch_arxiv_sage.py", line 507, in <module>
    main()
  File "full_and_pseudo_mini_batch_arxiv_sage.py", line 503, in main
    best_test = run(args, device, data)
  File "full_and_pseudo_mini_batch_arxiv_sage.py", line 273, in run
    batch_pred = model(blocks, batch_inputs)#------------*
  File "/home/cc/.local/lib/python3.6/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/cc/graph_partition_multi_layers/pseudo_mini_batch_full_batch/new_gp_SAGE/OGBN-arxiv/graphsage_model_arxiv.py", line 211, in forward
    x = self.layers[-1](blocks[-1], x)
  File "/home/cc/.local/lib/python3.6/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/cc/graph_partition_multi_layers/pseudo_mini_batch_full_batch/new_gp_SAGE/OGBN-arxiv/graphsage_model_arxiv.py", line 129, in forward
    graph.update_all(msg_fn, self._lstm_reducer)
  File "/home/cc/.local/lib/python3.6/site-packages/dgl/heterograph.py", line 4849, in update_all
    ndata = core.message_passing(g, message_func, reduce_func, apply_node_func)
  File "/home/cc/.local/lib/python3.6/site-packages/dgl/core.py", line 337, in message_passing
    ndata = invoke_udf_reduce(g, rfunc, msgdata, orig_nid=orig_nid)
  File "/home/cc/.local/lib/python3.6/site-packages/dgl/core.py", line 143, in invoke_udf_reduce
    bkt_rsts.append(func(nbatch))
  File "/home/cc/graph_partition_multi_layers/pseudo_mini_batch_full_batch/new_gp_SAGE/OGBN-arxiv/graphsage_model_arxiv.py", line 75, in _lstm_reducer
    _, (rst, _) = self.lstm(m, h)
  File "/home/cc/.local/lib/python3.6/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/cc/.local/lib/python3.6/site-packages/torch/nn/modules/rnn.py", line 582, in forward
    self.dropout, self.training, self.bidirectional, self.batch_first)
RuntimeError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 15.90 GiB total capacity; 14.81 GiB already allocated; 7.88 MiB free; 14.95 GiB reserved in total by PyTorch)
